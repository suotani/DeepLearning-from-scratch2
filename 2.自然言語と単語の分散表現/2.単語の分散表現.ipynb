{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語の分散表現\n",
    "\n",
    "色をRGBとして表現すると、ベクトルとしてのコンパクトな表現、似た色かどうかの定量的な測定ができるようになる。\n",
    "\n",
    "単語の意味についても同じように行えないか？\n",
    "この問いに答えるのが単語の分散表現。\n",
    "\n",
    "単語の分散表現は、単語を固定長ベクトルで表現する。\n",
    "\n",
    "## 分布仮説\n",
    "では、単語の意味をどのようにとらえるかだが、現在いろいろな方法が試されている中、多くの手法の中である共通のシンプルなアイデアに基づいていることがわかる。それが分布仮説。\n",
    "\n",
    "分布仮説とは、単語の意味は周囲の単語によって形成されるというもの\n",
    "\n",
    "つまり、単語自体に意味はなく、その単語のコンテキスト(文脈)によって意味が形成される。\n",
    "\n",
    "以下、コンテキストはより機械学習よりの意味として「注目している単語の周囲に存在する単語」を指すとする。\n",
    "\n",
    "どれくらい周囲まで見るのか(サイズ)はウィンドウサイズと呼ばれる。１であれば、単語の左右１つずつの単語のことを指す。\n",
    "\n",
    "## カウントベース手法(統計的手法)\n",
    "\n",
    "単語をベクトル化する最も単純な方法はカウントベース手法と呼ばれ、単語の周囲にどのような単語がどれくらい存在するのか数を数える手法である。\n",
    "\n",
    "たとえば、\"you say goodbye and i say hello.\"という文章があるとき、youについてみていくと、\n",
    "\n",
    "youの一つとなりにはsayが１つある。sayのとなりには、you, goodby, i, helloが1つずつある。youの隣にはyou, iなどが0個となる。\n",
    "\n",
    "これらをそれぞれの単語について集計し、行列にする。これを共起行列という。\n",
    "\n",
    "|-|you|say|goodbye|and|i|say|hello|.|\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|you|0|1|0|0|0|0|0|\n",
    "|say|1|0|1|0|1|1|0|\n",
    "|goodbye|\n",
    "|and|\n",
    "|i|\n",
    "|say|\n",
    "|hello|\n",
    "|.|\n",
    "\n",
    "これらの１行が単語それぞれのベクトルになる。\n",
    "\n",
    "それでは、共起行列をコーパスから作るメソッドを実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../../deep-learning-from-scratch-2\")\n",
    "from common.util import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size = 1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "            \n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "            \n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 類似度\n",
    "上記のメソッドとテキストをコーパスに変換するメソッドを使えば、テキストデータから単語を抜き出し、それぞれを整数値のベクトルに変換することができる。\n",
    "\n",
    "では、単語同士が似ているかどうかはどうやって測ればいいだろう。そのために、コサイン類似度を定義する・\n",
    "\n",
    "$$\n",
    "similarity(x, y) = \\frac{(x, y)}{\\|x\\| \\|y\\|} = ( \\frac{x}{\\|x\\|} , \\frac{y}{\\|y\\|} )\n",
    "$$\n",
    "\n",
    "つまり、ベクトルの内積をそれぞれのノルムで割るのである。もちろん、２次元であれば、２つのベクトルの間の角度に対するcosが出てくる。\n",
    "\n",
    "内積、ノルムはL2で計算する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ベクトルを正規化してから内積を取る\n",
    "# ゼロ割を防ぐために、小さな値(eps)を加算しておく\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y**2)) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70710676911547987"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テキストデータから単語間の類似度を計算する\n",
    "text = 'You say goodbye and i say hello.'\n",
    "corpus ,word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']]\n",
    "c1 = C[word_to_id['i']]\n",
    "cos_similarity(c0, c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コサイン類似度は-1～1の値をとるので、youとiは比較的類似度が高いと言える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ランキング表示\n",
    "類似度が計算できるようになったので、テキスト内の単語の中から、自分と似た単語をランキング形式で表示できるメソッドを実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query: 検索対象の単語\n",
    "# word_matrix:共起行列\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    if query not in word_to_id:\n",
    "        print(\"not found\")\n",
    "        return\n",
    "    \n",
    "    print('\\n[query] '+ query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "    \n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print('%s:  %s' % (id_to_word[i], similarity[i]))\n",
    "        \n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      "goodbye:  0.707106769115\n",
      "i:  0.707106769115\n",
      "hello:  0.707106769115\n",
      "say:  0.0\n",
      "and:  0.0\n"
     ]
    }
   ],
   "source": [
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

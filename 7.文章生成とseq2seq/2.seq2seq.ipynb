{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq\n",
    "動画や音声、言語データはなどは全て時系列データである。それらの時系列データから別の時系列データへ変換するような問題も多く考えられる。\n",
    "\n",
    "例えば機械翻訳や画像から説明テキストを生成するなど。\n",
    "\n",
    "時系列データを別の時系列データに変換するための手法として2つのＲＮＮを利用するseq2seqを見ていく。\n",
    "\n",
    "seq2seqはEncoder-Decoderモデルとも呼ばれる。\n",
    "\n",
    "例えば、日本語データ「吾輩は猫である。」をエンコーダーに入れ、その出力をデコーダーに入れると、英語データ「i am a cat」が出力される。\n",
    "\n",
    "<img src=\"encoder.png\">\n",
    "\n",
    "日本語データをエンコーダに入れ、最終的なoutputである$h$がエンコードされたデータである。\n",
    "\n",
    "そして日本語エンコーダの最終の隠れ状態$h$を英語文章生成モデルへの初めの隠れ状態として入力することで、変換が完了する、\n",
    "\n",
    "<img src=\"decoder.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 学習の進みが遅い場合の改善テクニック\n",
    "\n",
    "### 入力データの反転\n",
    "足し算問題を解くＡＩを考えたとする。\n",
    "\n",
    "入力は「5,7,+,5」で出力が「6,2」だとする。この時、反転とは入力データを「5,+,7,5」とする。\n",
    "\n",
    "単純だが、学習が早く進み、性能も良くなることが分かっている。\n",
    "\n",
    "理論的になぜうまくいくのか分かっていないが、入力の初めは逆伝搬からは遠いため、途中様々な影響を受けて到達する。反転させることで、評価の影響をダイレクトに受けることができ、入力データに対する勾配がうまく伝達するからだと考えられている。\n",
    "\n",
    "### のぞき見\n",
    "デコーダが受け取る翻訳情報としては、$h$のみになる。現状ではＬＳＴＭのみがそのhを受け取っていることになる。そこで、重要な情報が詰まったhを他の層にも与えることができないかと考える。\n",
    "\n",
    "<img src=\"peeky.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
